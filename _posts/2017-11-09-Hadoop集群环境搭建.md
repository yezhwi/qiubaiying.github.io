---
layout:     post
title:      Hadoop集群环境搭建
subtitle:   
date:       2017-11-09
author:     Yezhiwei
header-img: img/WechatIMG38.jpeg
catalog: true
category: BigData
tags:
    - Hadoop
    - BigData
---


## 配置虚拟机CentOS

* 修改IP

```
cd /etc/sysconfig/network-scripts

sudo vim ifcfg-eth0
```

* 重启服务

```
sudo /etc/init.d/network restart
```

* 两台虚拟机使用相同的网卡，需要重新安装网卡

> 通过 ifconfig查看，虚拟机使用相同的网卡eth0,所以需要卸载，在vmware设置中移除[网络适配器]，然后再点添加一个新的[网络适配器]，再通过ifconfig查看 ，已经变成eth1了，再通过curl www.baidu.com 命令就能上网了

## Hadoop

* 上传hadoop-1.2.1-bin.tar.gz 到服务器/usr/local/上，并解压tar -xvzf hadoop-1.2.1-bin.tar.gz
* 在 /usr/local/hadoop-1.2.1 目录下创建tmp目录，用于保存使用过程中产生的临时文件
* conf 目录下修改masters、slaves文件

> 在masters文件中添加一行 master
```
root@ubuntu238:/usr/local/hadoop-1.2.1/conf# cat masters
master
```

> 在slaves文件中添加两行 slave1 slave2
```
root@ubuntu238:/usr/local/hadoop-1.2.1/conf# cat slaves
slave1
slave2
```

* 修改core-site.xml文件

```
<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/usr/local/hadoop-1.2.1/tmp</value>
	</property>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://master:9000</value>
	</property>
</configuration>
```

* 修改mapred-site.xml

```
<configuration>
	<property>
		<name>mapred.job.tracker</name>
		<value>http://master:9001</value>
	</property>
</configuration>
```

* 修改hdfs-site.xml文件,配置数据在hdfs中存储的副本数，默认是3

```
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
	</property>
</configuration>
```

* 修改hadoop-env.sh文件，添加环境变量

```
export JAVA_HOME=/usr/local/java
```

* 配置Hosts

```
192.168.111.38 master
192.168.111.39 slave1
192.168.111.40 slave2
```

* 修改hostname

```
Ubuntu
vim /etc/hostname

CentOS
vim /etc/sysconfig/network

hostname xxx  #临时设定hostname为xxx
```

* 将hadoop配置后的文件复制到两台slave节点上

```
scp  -rp  hadoop-1.2.1  192.168.111.xx:/usr/local/

hosts文件和hostname分别配置
```

* 查看防火墙状态及关闭

```
service iptables status或iptables -L

/etc/init.d/iptables stop
```

* 设置selinux，网络传输过程中，防火墙和selinux都可能会导致意想不到的问题。 通过setenforce 0设置，设置完之后执行getenforce，结果应该为permissive。

* ssh互信

```
对每台机器执行
ssh-keygen
cd ~/.ssh/ #进入这个目录下可以看到公钥和私钥文件。
cat id_rsa.pub >authorized_keys

将slave1和slave2中的公钥拷贝到master节点的authorized_keys中，然后将master节点的authorized_keys拷贝到slave1和slave2节点
```

* 格式化namenode：在bin目录下执行`./hadoop namenode -format`

* 启动集群：`./start-all.sh`

* jps命令验证进程

* 验证hdfs是否可以用

```
./hadoopfs -ls /   #查看当前hdfs下有啥
./hadoopfs -put /etc/passwd  /  #往集群中上传passwd文件
./hadoop fs-cat /passwd           #读hadoop中的passwd文件
```

* 配置环境变量

```
vim  ~/.bashrc
export JAVA_HOME=/usr/local/java
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib
export HADOOP_HOME=/usr/local/hadoop-1.2.1export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
```









