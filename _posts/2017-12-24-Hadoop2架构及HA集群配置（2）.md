---
layout:     post
title:      Hadoop2.0架构及HA集群配置（2）
subtitle:   HA集群配置搭建
date:       2017-12-24
author:     Yezhiwei
header-img: img/WechatIMG38.jpeg
catalog: true
category: BigData
tags:
    - BigData
    - Yarn
    - Hadoop
    - Zookeeper
---

### 背景

1. 在Hadoop2.0中通常由两个NameNode组成，一个处于Active状态，另一个处于Standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。

2. Hadoop2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置大于或等于3奇数个JournalNode。

3. 需要配置一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为Active状态。

4. Hadoop2.4之前的版本ResourceManager只有一个，仍然存在单点故障，Hadoop-2.4.1解决了这个问题，有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调。

### 基础软件安装

* JDK 1.7
* Zookeeper 3.4.9
* Hadoop 2.6.1

### Hadoop（HDFS HA）总体架构

![Hadoop（HDFS HA）总体架构](/img/hadoop2-4.png)


### 服务器分布及相关说明

由于目前只有3台虚拟机，所以分配如下：

| 服务器角色/服务器 | Master(192.168.111.238) | Slave1(192.168.111.239) | Slave2(192.168.111.240) | 
| ---- | :----: | :----: | :----: |
| NameNode | Y | Y | N |
| ResourceManager | Y | Y | N |
| DFSZKFailoverController(zkfc) | Y | Y | N |
| DataNode | Y | Y | Y |
| DataManager | Y | Y | Y |
| JournalNode | Y | Y | Y |
| Zookeeper | Y | Y | Y |
| QuorumPeerMain | Y | Y | Y |

### 安装步骤

* JDK安装
* SSH免密登录
* hosts配置

	>192.168.111.238 master
	>
	>192.168.111.239 slave1
	>
	>192.168.111.240 slave2

* 在Master上配置并启动ZooKeeper

	1. 将安装包Zookeeper上传到/usr/local/目录下，然后解压/usr/local/zookeeper-3.4.9
	2. 然后在目录/usr/local/zookeeper-3.4.9中新建数据目录和日志目录 `mkdir -p /usr/local/zookeeper-3.4.9/{data,datalog}`
	3. `cp /usr/local/zookeeper-3.4.9/conf/zoo_sample.cfg /usr/local/zookeeper-3.4.9/conf/zoo.cfg` 并进行如下配置：

	>tickTime=2000
	
	>initLimit=10

	>syncLimit=5

	>dataDir=/usr/local/zookeeper-3.4.9/data
	
	>dataLogDir=/usr/local/zookeeper-3.4.9/datalog

	>clientPort=2181
	
	>server.1= master:2888:3888
	
	>server.2= slave1:2888:3888
	
	>server.3= slave2:2888:3888
	
	4. 在Master上执行下面命令：

	> echo 1 > /usr/local/zookeeper-3.4.9/data/myid
	
	5. 然后将Master上的`/usr/local/zookeeper-3.4.9`目录拷贝到slave1和slave2上，然后分别在两台机器上执行 

	`echo 2 > /usr/local/zookeeper-3.4.9/data/myid` 和 `echo 3 > /usr/local/zookeeper-3.4.9/data/myid`
	
	6. 然后分别启动服务

	启动服务 ` ./bin/zkServer.sh start` 
	
	查看状态 ` ./bin/zkServer.sh status` 
	
	7. jps 查看进程中有 `QuorumPeerMain`
	
	8. 按步骤6在两台从服务器上启动zookeeper服务。 

* Hadoop（HDFS HA）集群部署
	* 解压tar -xzvf hadoop-2.6.1.tar.gz
	* `cd /usr/local/hadoop-2.6.1/etc/hadoop` 配置 `hadoop-env.sh` 和 `yarn-env.sh` 的JAVA_HOME
	* slaves文件配置从节点

	> slave1
	>
	> slave2
	
	* core-site.xml配置
	
	```
	<configuration>
		<!-- 这里的值指的是默认的HDFS路径。当有多个HDFS集群同时工作时，集群名称在这里指定！该值来自于hdfs-site.xml中的配置 -->
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://masters</value>
	    </property>
		<!-- 这里的路径默认是NameNode、DataNode、JournalNode等存放数据的公共目录。用户也可以自己单独指定这三类节点的目录。 -->	
	    <property>
	        <name>hadoop.tmp.dir</name>
	        <value>/usr/local/hadoop-2.6.1/tmp</value>
	    </property>
		<!-- ZooKeeper集群的地址和端口。注意，数量一定是奇数，且不少于三个节点 -->
	    <property>
	        <name>ha.zookeeper.quorum</name>
	        <value>master:2181,slave1:2181,slave2:2181</value>
	    </property>
	</configuration>
	```
	
	* hdfs-site.xml配置

	```
	<configuration>
		<!-- 指定DataNode存储block的副本数量。默认值是3个 -->
	    <property>
	        <name>dfs.replication</name>
	        <value>3</value>
	    </property>
	    <!--使用federation时，HDFS集群别名。名字可以随便起，多个集群时相互不重复即可,指定hdfs的nameservice为masters，需要和core-site.xml中的保持一致 -->
	    <property>
	        <name>dfs.nameservices</name>
	        <value>masters</value>
	    </property>
	    <!-- 该集群的namenode的机器。下面有两个NameNode，分别是Master，Slave1 -->
	    <property>
	        <name>dfs.ha.namenodes.masters</name>
	        <value>master,slave1</value>
	    </property>
	    <!-- Master的RPC通信地址 -->
	    <property>
	        <name>dfs.namenode.rpc-address.masters.master</name>
	        <value>master:9000</value>
	    </property>
	    <!-- Master的http通信地址 -->
	    <property>
	        <name>dfs.namenode.http-address.masters.master</name>
	        <value>master:50070</value>
	    </property>
	    <!-- Slave1的RPC通信地址 -->
	    <property>
	        <name>dfs.namenode.rpc-address.masters.slave1</name>
	        <value>slave1:9000</value>
	    </property>
	    <!-- Slave1的http通信地址 -->
	    <property>
	        <name>dfs.namenode.http-address.masters.slave1</name>
	        <value>slave1:50070</value>
	    </property>
	    <!-- 指定集群的两个NameNode共享edits文件目录时，使用的JournalNode集群信息。指定NameNode的元数据在JournalNode上的存放位置 -->
	    <property>
	        <name>dfs.namenode.shared.edits.dir</name>
	        <value>qjournal://master:8485;slave1:8485;slave2:8485/masters</value>
	    </property>
	    <!-- 指定JournalNode在本地磁盘存放数据的位置 -->
	    <property>
	        <name>dfs.journalnode.edits.dir</name>
	        <value>/usr/local/hadoop-2.6.0/journal</value>
	    </property>
	    <!-- 开启NameNode失败自动切换 -->
	    <property>
	        <name>dfs.ha.automatic-failover.enabled</name>
	        <value>true</value>
	    </property>
	    <!-- 配置失败自动切换实现方式 -->
	    <property>
	        <name>dfs.client.failover.proxy.provider.masters</name>
	        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	    </property>
	    <!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行-->
	    <property>
	        <name>dfs.ha.fencing.methods</name>
	        <value>
	            sshfence
	            shell(/bin/true)
	        </value>
	    </property>
	    <!-- 使用sshfence隔离机制时需要ssh免登陆 -->
	    <property>
	        <name>dfs.ha.fencing.ssh.private-key-files</name>
	        <value>/root/.ssh/id_rsa</value>
	    </property>
	    <!-- 配置sshfence隔离机制超时时间 -->
	    <property>
	        <name>dfs.ha.fencing.ssh.connect-timeout</name>
	        <value>30000</value>
	    </property>
	</configuration>
	```
	
	* mapred-site.xml 配置
	
	```
	<configuration>
		<!-- 指定运行mapreduce的环境是yarn -->
	    <property>
	        <name>mapreduce.framework.name</name>
	        <value>yarn</value>
	    </property>
	</configuration>
	```
	
	* yarn-site.xml 配置
	
	```
	<configuration>
	    <!-- 开启RM高可靠 -->
	    <property>
	        <name>yarn.resourcemanager.ha.enabled</name>
	        <value>true</value>
	    </property>
	    <!-- 指定RM的cluster id -->
	    <property>
	        <name>yarn.resourcemanager.cluster-id</name>
	        <value>RM_HA_ID</value>
	    </property>
	    <!-- 指定RM的名字 -->
	    <property>
	        <name>yarn.resourcemanager.ha.rm-ids</name>
	        <value>rm1,rm2</value>
	    </property>
	    <!-- 分别指定RM的地址 -->
	    <property>
	        <name>yarn.resourcemanager.hostname.rm1</name>
	        <value>master</value>
	    </property>
	    <property>
	        <name>yarn.resourcemanager.hostname.rm2</name>
	        <value>slave1</value>
	    </property>
	    <property>
	        <name>yarn.resourcemanager.recovery.enabled</name>
	        <value>true</value>
	    </property>
	    <property>
	        <name>yarn.resourcemanager.store.class</name>
	        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
	    </property>
	    <!-- 指定zk集群地址 -->
	    <property>
	        <name>yarn.resourcemanager.zk-address</name>
	        <value>master:2181,slave1:2181,slave2:2181</value>
	    </property>
	    <property>
	        <name>yarn.nodemanager.aux-services</name>
	        <value>mapreduce_shuffle</value>
	    </property>
	</configuration>
	```
	
* 启动过程
	
	* 三台机器上分别启动journalnode 
	
	`./sbin/hadoop-daemon.sh start journalnode`， 
	
	并jps查看进程，显示 `JournalNode`
	
	* 格式化namenode 

	`./bin/hdfs namenode -format`
	
	格式化ZKFC `./bin/hdfs zkfc -formatZK`
	
	* 启动主NameNode 

	`./sbin/hadoop-daemon.sh start namenode`
	
	* 在备NameNode从master同步到slave1  

	`./bin/hdfs namenode -bootstrapstandby` 检查 tmp下生成dfs
	
	* 启动备NameNode 

	`./sbin/hadoop-daemon.sh start namenode`
	
	* 在两个NameNode节点（master、slave）上执行 

	`./sbin/hadoop-daemon.sh start zkfc`
	
	* 启动所有的DataNode节点

	`./sbin/hadoop-daemon.sh start datanode`
	
	* 启动Yarn 

	`./sbin/start-yarn.sh`
	
	* HDFS的HA功能测试 http://192.168.111.238:50070/dfshealth.html

	![active](/img/ha-1.png)
	
	![standby](/img/ha-2.png)
	
	* 将主节点上的NameNode kill 后，再次重启查看页面效果。
	
	








